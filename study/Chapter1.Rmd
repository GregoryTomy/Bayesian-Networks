---
title: "1 Bayesian Networks: Discrete Case Notebook"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

# Graphical Representation

```{r}
library(bnlearn)
```

We first create a DAG with one node for each of the variables and no
arcs

```{r}
dag <- empty.graph(nodes = c("A", "S", "E", "O", "R", "T"))
dag
```

Next, we add the arcs that encode the direct dependencies between the
variables in the survey. Age and Sex are not influenced by any other
variables. But both have a direct influence on Education. Similarlly,
Sex also influences Education. In turn, Education influences both
Occupation and Residence. Finally, the preferred menas of transport are
directly influenced by both Occupation and Residence.

```{r}
dag <- set.arc(dag, from = "A", to = "E")
dag <- set.arc(dag, from = "S", to = "E")
dag <- set.arc(dag, from = "E", to = "R")
dag <- set.arc(dag, from = "E", to = "O")
dag <- set.arc(dag, from = "O", to = "T")
dag <- set.arc(dag, from = "R", to = "T")
dag
```

```{r}
print(modelstring(dag))
print(nodes(dag))
print(arcs(dag))
```
```{r}
graphviz.plot(dag)
```

```{r}
dag2 <- empty.graph(nodes = c("A", "S", "E", "O", "R", "T"))
arc.set <- matrix(c("A", "E",
                    "S", "E",
                    "E", "O",
                    "E", "R",
                    "O", "T",
                    "R", "T"),
                  byrow = TRUE, ncol = 2,
                  dimnames = list(NULL, c("from", "to")))
arcs(dag2) <- arc.set
all.equal(dag, dag2)
```

# Probabilisitc Representation

```{r}
A.lv <- c("young", "adult", "old")
S.lv <- c("M", "F")
E.lv <- c("high", "uni")
O.lv <- c("emp", "self")
R.lv <- c("small", "big")
T.lv <- c("car", "train", "other")
```

## One Dimensional Tables

```{r}
A.prob <- array(c(0.30, 0.50, 0.20), dim = 3,
                dimnames = list(A = A.lv))
print(A.prob)
S.prob <- array(c(0.60, 0.40), dim = 2,
                dimnames = list(S = S.lv))
print(S.prob)
```

## Two Dimensional Tables

```{r}
O.prob <- array(c(0.96, 0.04, 0.92, 0.08), dim = c(2, 2),
                dimnames = list(O = O.lv, E = E.lv))
R.prob <- array(c(0.25, 0.75, 0.20, 0.80), dim = c(2, 2),
                dimnames = list(R = R.lv, E = E.lv))
print(O.prob)
print(R.prob)
```

## Three Dimensional Tables

```{r}
E.prob <- array(c(0.75, 0.25, 0.72, 0.28, 0.88, 0.12, 0.64,
                  0.36, 0.70, 0.30, 0.90, 0.10), dim = c(2, 3, 2),
                dimnames = list(E = E.lv, A = A.lv, S = S.lv))
T.prob <- array(c(0.48, 0.42, 0.10, 0.56, 0.36, 0.08, 0.58,
                  0.24, 0.18, 0.70, 0.21, 0.09), dim = c(3, 2, 2),
                dimnames = list(T = T.lv, O = O.lv, R = R.lv))
# print(E.prob)
print(T.prob)
```

# Model formula interface

```{r}
dag3 <- model2network("[A][S][E|A:S][O|E][R|E][T|O:R]")
all.equal(dag, dag3)
```

```{r}
# list of local distributions 
cpt <- list(A = A.prob, S = S.prob, E = E.prob, 
            O = O.prob, R = R.prob, T = T.prob)

# combine DAG with cpt
bn <- custom.fit(dag, cpt)
print(nparams(bn))
print(arcs(bn))
print(bn$R)
A.lv
```

# Estimating Parameters: Condiitonal Probability Tables
## From Data

```{r}
survey <- read.table("survey.txt", header = TRUE)
survey <- data.frame(lapply(survey, as.factor))

head(survey)
```

## Frequentist and Maximum likelihood Estimates

bn.fit constructs a BN using the data. custom.fit used before does so
using a set of custom parameters specified by the user.

```{r}
bn_mle <- bn.fit(dag2, data = survey, method = "mle")
print(bn_mle$O)
prop.table(table(survey[, c("O", "E")]), margin = 2)
```

## Bayesian Estimates

The estimated posterior probabilities are computed from a uniform prior
over each conditional probability table.

iss -\> imaginary sample size or equivalent sample size - determines how
much weight is assigned to the prior distribution compared to the data
when computing. \# the posterior.

```{r}

bn_bayes <- bn.fit(dag2, data = survey, method = "bayes", iss=10) 
print(bn_bayes$O)

bn_bayes <- bn.fit(dag2, data = survey, method = "bayes", iss=20) 
print(bn_bayes$O)

bn_bayes <- bn.fit(dag2, data = survey, method = "bayes", iss=100) 
print(bn_bayes$O)

bn_bayes <- bn.fit(dag2, data = survey, method = "bayes", iss=10000) 
print(bn_bayes$O)
```

Increasing the value of iss makes the posterior distribution more and
more flat, pushing it towards the uniform distribution used as the
prior.

# Learning the DAG Structure: Tests and Scores

the DAG of the survey we are using as an example suggests that train
fares should be adjusted (to maximise profit) on the basis of Occupation
and Residence alone.

## Conditional Independence Tests

Conditional independence tests focus on the presence of individual arcs.
Since each arc encodes a probabilistic dependence, conditional
independence tests can be used to assess whether that probabilistic
dependence is supported by the data. If the null hypothesis (of
conditional independence) is rejected, the arc can be considered for
inclusion in the DAG.

```{r}
# degress of freedom
(nlevels(survey[, "T"]) - 1) * (nlevels(survey[, "E"]) - 1) *
+ (nlevels(survey[, "O"]) * nlevels(survey[, "R"]))
```

## ci.test implements both log-likelihood ratio $G^2$ and Pearson's $X^2$

```{r}
# log-likelihood ratio G^2
ci.test(
  "T", "E", c("O", "R"), test = "mi", data = survey
)
```

```{r}
ci.test(
  "T", "E", c("O", "R"), test = "x2", data = survey
)
```

Both tests return very large p-values indicating that the dependence
between E and T is not supported by the data. Next, we can check whether
one of the arcs in the DAG should be removed because the dependence
relationship is not suuported by the data. We check O to T.

```{r}
ci.test(
  "T", "O", "R", test = "x2", data = survey
)
```

```{r}
ci.test(
  "T", "O", test = "x2", data = survey
)
```

We can check the strength of all arcs using arc.strength. arc.strength
measures the strength of the probabilistic dependence by removing each
arc from the graph and quantifying the change with some probabilistic
criterion. Strength here is p-value. We see that all arcs are well
supported by the data except O to T.

```{r}
arc.strength(
  dag, data = survey, criterion = "x2"
)
```

## Network Scores

Network scores consider the whole DAG. They are goodness-of-fit
statistics measuring how well the DAG fits the dependence structure of
the data. There are two network scores we consider: 1. Bayesian
Information Criterion (BIC) 2. Bayesian Dirichlet equivalent form (BDe)

```{r}
score(dag, data = survey, type = 'bic')
# log Bde
score(dag, data = survey, type = 'bde', iss = 10)
# remember iss here is the imaginary sample size - weights assigned to the flat
# prior distribution in ters of the size of an imaginary sample.
score(dag, data = survey, type = 'bde', iss = 1) # small iss yields the same score as bic
```

We can generate random DAGs and compare it to previous DAGs thropugh it
score

```{r}
rnd <- random.graph(nodes = c("A", "S", "E", "O", "R", "T"))
modelstring(rnd)
score(rnd, data = survey, type = "bic")
```

## Learning the DAG

There are several algorithms that tackle the task of learning the DAG
from the data. \### Hill Climbing Starts a DAG with no arcs and then
adds, removes, and reverses one arc at at time and picks the change that
increase the score the most.

```{r}
learned <- hc(survey)
modelstring(learned)
score(learned, data = survey, type = "bic")
```

We can check how removing each arc affects the network score by using
the arc.strength function witha network score criterion

```{r}
arc.strength(learned, data = survey, criterion = "bic")
```

Here removal of any arc decreases the score further

```{r}
arc.strength(dag, data = survey, criterion = "bic")
```

We see how removing O to T increases the score the most which is
consistent with out earlier high p-value.

# Using Discrete BNs

We say that two variables **X** and **Y** are conditionally independent
or *d-separated* given **Z** if there are no arcs connecting them that
are not blocked by the conditioning variable.

Note: if all paths between **X** and **Y** are blocked, we say they are
conditionally independent. The converse is not necessarily true. Not all
conditional independence relationships are reflected on a graph

```{r}
dsep(dag, x = "S", y = "R") # false when not conditioned on E
dsep(dag, x = "O", y = "R")

```
## Fundamental Connections
```{r}
path.exists(dag, from = "S", to = "R") 
# serial connection
dsep(dag, x = "S", y = "R", z = "E") # true when conditioned on E
# divergent connection 
dsep(dag, x = "O", y = "R", z = "E") # true when conditioned on E
# convergent connection
dsep(dag, x = "A", y = "S", z = "E")
```
### Convergent connection
Conditioning on particular node can also make other nodes dependent when they are
marginally independent
```{r}
# convergent connection
dsep(dag, x = "A", y = "S")
dsep(dag, x = "A", y = "S", z = "E")
```
## Using Conditonal Probability Tables
### Exact Inference
```{r}
library(gRain)

# as.grain creates the junction tree
# compile builds the probability table of the junction tree
junction <- compile(as.grain(bn))
```
For example, we may be interested in the attitudes of women towards car
and train use compared to the whole survey sample

```{r}
# corrsponds to P(T)
querygrain(junction, nodes = "T")$T
jsex <- setEvidence(
  junction, 
  nodes = "S",
  states = "F"
)
# corresponds to P(T | S = F)
querygrain(jsex, nodes = "T")$T
```
There is no real difference in probabilities derived from the junction 
before and after inputting evidence. This suggests women show about the 
same preference towards car, train, and other, as the survey as a whole.

Another interesting question is how living in a small city affects car and
train use, that is P(T | R = small). We can conjecture that people working
in cities often live in neighboring towns and commute to their workplaces.
This forces them to travel mostly by car or train and less so by other 
means of transporation like bicycles or bus. 
```{r}
jres <- setEvidence(
  junction, 
  nodes = "R",
  states = "small"
)
querygrain(jres, nodes = "T")$T
```
We see that this reasoning is supported by the BN. Combined probabilty 
of car and train is at 90% (for people living in small cities.)

Conditional probability queries can be used to determine conditional independence. Consider again the relationship between S and T, this 
time conditioning on E = high. That is we want P(S, T | E = high).
```{r}
jedu <- setEvidence(
  junction,
  nodes = "E",
  states = "high"
)
SxT.cpt <-querygrain(junction, nodes = c("S", "T"), type = "joint")
SxT.cpt

querygrain(junction, nodes = c("S", "T"), type ="marginal")
# P(S|T = t, E = high)
querygrain(junction, nodes = c("S", "T"), type ="conditional")
```
We note that all the conditional probabilities P(S = M| T= t, E = high) are virtually the same regardless of T - the same when S = F. This suggests that S and T are conditionally independent on E. That is, knowing the sex of the person is not informative of their travel preferences when we know their Education. We can also check this with graphical separation.
```{r}
dsep(bn, x = "S", y = "T", z = "E")
```
Another way of confirming connditional independence is to use the joint distribution and perform a Pearsons $X^2$ test for independence. First, we need to multiply the cpt by the sample size to convert conditional probability into contingency tables.
```{r}
SxT.ct = SxT.cpt * nrow(survey)
chisq.test(SxT.ct)
```
### Approximate Inference
In this approach we use Monte Carlo simulations to randomly generate
observations from the BN and use these observations to approximate the 
conditional probabilities we are interested in.

For Discrete BNs we can use rejection sampling.
```{r}
# random sampling
# return the probability of a specific event given some evidence
cpquery(
  bn,
  event = (S == "M") & (T == "car"),
  evidence = (E == "high"),
  n = 10^6  # num of random observations, increase to improve apprx
)
```
Increasing precision by increasing n has several drawbacks - longer query times and the precision may still be low if the probability of the evidence is low. A better approach is *likelihood weighting* - where random observations generated match the evidence and re-weights them appropriately when computing the conditional probability query. 
```{r}
cpquery(bn, 
        event = (S == "M") & (T == "car"),
        evidence = list(E = "high"), 
        method = "lw"
)
```
```{r}
cpquery(
  bn,
  event = (S == "M") & (T == "car"),
  evidence = ((A == "young") & (E == "uni") | (A == "adult"))
)
```
The implementation of likelihood weighting in cpquery is not flexible enough to compute a query with composite evidence like the above; in that respect it shares the same limitations as the functions in the
gRain package.
```{r}
SxT <- cpdist(
  bn,
  nodes = c("S", "T"),
  evidence = (E == "high")
)
head(SxT)
```
Comparing obervations contained in the above data frame to that produced
by querygrain
```{r}
# produce contingency table from SxT dataframe
# table(SxT)
# convert to probabilities
prop.table(table(SxT))

SxT.cpt

```
  # Plotting BNs
## Plotting Graphs
```{r}
graphviz.plot(dag, layout = "dot")
graphviz.plot(dag, layout = "fdp")
graphviz.plot(dag, layout = "circo")
```
```{r}
hlight <- list(
  nodes = nodes(dag),
  arcs = arcs(dag),
  col = "grey",
  textCol = "grey"
)

pp <- graphviz.plot(dag, highlight = hlight)
```
```{r}
library(Rgraphviz)
# rendering arcs
edgeRenderInfo(pp) <-
  list(col = c("S~E" = "red", "E~R" = "black"),
       lwd = c("S~E" = 3, "E~R" = 5))

# highlighting nodes
nodeRenderInfo(pp) <- list(
  col = c("S" = "blue", "E" = "black", "R" = "black"),
  textCol = c("S" = "black", "E" = "purple", "R" = "black"),
  fill = c("E" = "grey")
)
renderGraph(pp)
```
## Plotting Conditional Probability Distributions
```{r}
bn.fit.barchart(
  bn_mle$T, main = "Travel",
  xlab = "Pr(T | R, O)", ylab = ""
)

bn.fit.dotplot(
  bn_mle$T, main = "Travel",
  xlab = "Pr(T | R, O)", ylab = ""
)
```
```{r}
Evidence <- factor(
  c(rep("Unconditional", 3), rep("Female", 3), rep("Small City", 3)),
  levels = c("Unconditional", "Female", "Small City")
)
Evidence

Travel <- factor(
  rep(c("car", "train", "other"), 3),
  levels = c("other", "train", "car")
)
Travel

distr <- data.frame(
  Evidence = Evidence, 
  Travel = Travel,
  Prob = c(0.5618, 0.2808, 0.15730, 0.5620, 0.2806,
           0.1573, 0.4838, 0.4170, 0.0990)
)

distr
```
```{r}
library(lattice)

barchart(
  Travel ~ Prob | Evidence, data = distr,
  layout = c(3, 1), xlab = "probability",
  scales = list(alternating = 1, tck = c(1, 0)),
  strip = strip.custom(
    factor.levels =c(expression(Pr(T)),
                     expression(Pr({T} * " | " * {S == F})),
                     expression(Pr({T} * " | " * {R == small})))),
  panel = function(...) {
    panel.barchart(...)
    panel.grid(h = 0, v = -1)
}
)
```
# Exercise 1.3
```{r}
# set the nodes
my_dag <- empty.graph(nodes = c("A", "S", "E", "O", "R", "T"))

# set arcs
arc_matrix <- matrix(c("A", "E",
                    "S", "E",
                    "E", "O",
                    "E", "R",
                    "O", "T",
                    "R", "T"),
                  byrow = TRUE, ncol = 2,
                  dimnames = list(NULL, c("from", "to")))
arcs(my_dag) <- arc_matrix

my_dag

# easier way
my_dag_2 <- model2network("[A][S][E|A:S][O|E][R|E][T|O:R]")
my_dag_2
```
```{r}
library(purrr)

arcs(my_dag_2)
nodes(my_dag_2)

pars <-  map(nodes(my_dag_2), parents, x = my_dag_2)
pars

childs <- map(
  nodes(my_dag_2),
  children,
  x = my_dag_2
)
childs
```


```{r}
modelstring(my_dag_2)
```

```{r}
# # read in the data
survey2 <- read.table("survey.txt", header = TRUE)
survey2 <- data.frame(map(survey2, as.factor))

fitted <- bn.fit(my_dag_2, survey2, method = "bayes")

my_dag_3 <- drop.arc(my_dag_2, from = "E", to = "O")

fitted_2 <- bn.fit(my_dag_3, survey2, method = "bayes")
```

```{r}
fitted
```

```{r}
fitted_2
```














